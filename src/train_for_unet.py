import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

# åŒ¯å…¥ Dataset å’Œ U-Net
from src.data.dataset_pairs import UpscaleDataset
from src.models.unet_sr import UNetSR

from src.config.train_config import TrainConfig as cfg

def _build_exp_prefix() -> str:
    """
    æ ¹æ“š TrainConfig è‡ªå‹•ç”¢ç”Ÿå¯¦é©—åç¨±ã€‚
    å¦‚æœ cfg.exp_name æœ‰å¡«ï¼Œå°±ä½¿ç”¨æ‰‹å‹•æŒ‡å®šçš„åç¨±ã€‚
    """
    if cfg.exp_name is not None:
        return cfg.exp_name

    # è‡ªå‹•çµ„åï¼šunet_ps32_bs16_lr1e-4 é€™ç¨®æ ¼å¼
    model = cfg.model_name
    ps = cfg.patch_size if cfg.patch_size is not None else "full"
    bs = cfg.batch_size

    # lr â†’ 1e-4 é€™ç¨®å­—ä¸²
    lr_str = f"{cfg.learning_rate:.0e}".replace("-0", "-")  # 1e-04 -> 1e-4

    return f"{model}_ps{ps}_bs{bs}_lr{lr_str}"

def train():
    # --- åƒæ•¸è¨­å®š (U-Net æ¯”è¼ƒåƒé¡¯å­˜ï¼ŒBatch Size å¯èƒ½è¦å°ä¸€é») ---
    LR_DIR = cfg.lr_dir
    HR_DIR = cfg.hr_dir
    PATCH_SIZE = cfg.patch_size
    
    BATCH_SIZE = cfg.batch_size
    LEARNING_RATE = cfg.learning_rate
    NUM_EPOCHS = cfg.num_epochs          # U-Net æ”¶æ–‚æ¯”è¼ƒæ…¢ï¼Œçµ¦å®ƒå¤šä¸€é»æ™‚é–“
    
    cfg.model_name = "unet"
    
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print(f"ğŸš€ Training UNetSR on {DEVICE}...")

    # --- æº–å‚™è³‡æ–™ ---
    if not os.path.exists(LR_DIR) or not os.path.exists(HR_DIR):
        print("âŒ æ‰¾ä¸åˆ°è³‡æ–™å¤¾")
        return

    # num_workers=2 åŠ é€Ÿè®€å–
    dataset = UpscaleDataset(lr_dir=LR_DIR,
                             hr_dir=HR_DIR,
                             patch_size=PATCH_SIZE,
                             scale_factor=4)
    
    if len(dataset) == 0:
        print("âŒ Dataset æ˜¯ç©ºçš„")
        return
    
    train_loader = DataLoader(dataset,
                              batch_size=BATCH_SIZE,
                              shuffle=True,
                              num_workers=cfg.num_workers,
                              pin_memory=True)
            
    prefix = _build_exp_prefix()

    # --- å»ºç«‹æ¨¡å‹ ---
    model = UNetSR().to(DEVICE)
    
    # ä½¿ç”¨ L1 Loss (æ¯” MSE æ›´èƒ½ç”¢ç”ŸéŠ³åˆ©é‚Šç·£)
    criterion = nn.L1Loss()
    
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # å­¸ç¿’ç‡æ’ç¨‹ï¼šæ¯ 50 è¼ªè¡°æ¸›ä¸€åŠ
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)

    os.makedirs(cfg.checkpoint_dir, exist_ok=True)

    # --- è¨“ç·´è¿´åœˆ ---
    model.train()
    for epoch in range(NUM_EPOCHS):
        epoch_loss = 0.0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}")
        
        for lr_imgs, hr_imgs in progress_bar:
            lr_imgs, hr_imgs = lr_imgs.to(DEVICE), hr_imgs.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(lr_imgs)
            loss = criterion(outputs, hr_imgs)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            
            # é¡¯ç¤ºç•¶å‰ Loss å’Œ Learning Rate
            current_lr = optimizer.param_groups[0]['lr']
            progress_bar.set_postfix({'loss': f"{loss.item():.6f}", 'lr': f"{current_lr:.6f}"})
            
        # æ›´æ–°å­¸ç¿’ç‡
        scheduler.step()
        
        
        # æ¯ 20 è¼ªå­˜æª”ä¸€æ¬¡
        if (epoch + 1) % cfg.save_every == 0:
            save_path = os.path.join(
                cfg.checkpoint_dir,
                f'{prefix}_epoch{epoch+1}.pth'
            )
            torch.save(model.state_dict(), save_path)

    # --- æœ€çµ‚å­˜æª” ---
    final_path = os.path.join(
        cfg.checkpoint_dir,
        f'{prefix}_final.pth'
    )
    torch.save(model.state_dict(), final_path)
    print(f"ğŸ‰ UNet Training Finished! Saved to {final_path}")

if __name__ == "__main__":
    train()