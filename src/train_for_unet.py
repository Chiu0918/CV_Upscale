import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

# åŒ¯å…¥ Dataset å’Œ U-Net
from src.data.dataset_pairs import UpscaleDataset
from src.models.unet_sr import UNetSR

def train():
    # --- åƒæ•¸è¨­å®š (U-Net æ¯”è¼ƒåƒé¡¯å­˜ï¼ŒBatch Size å¯èƒ½è¦å°ä¸€é») ---
    LR_DIR = 'data/train_lr'
    HR_DIR = 'data/train_hr'
    
    BATCH_SIZE = 16            # å¦‚æœ 8 è·‘ä¸å‹• (Out of Memory)ï¼Œè«‹æ”¹æˆ 4
    LEARNING_RATE = 1e-4
    NUM_EPOCHS = 50          # U-Net æ”¶æ–‚æ¯”è¼ƒæ…¢ï¼Œçµ¦å®ƒå¤šä¸€é»æ™‚é–“
    
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print(f"ğŸš€ Training UNetSR on {DEVICE}...")

    # --- æº–å‚™è³‡æ–™ ---
    if not os.path.exists(LR_DIR) or not os.path.exists(HR_DIR):
        print("âŒ æ‰¾ä¸åˆ°è³‡æ–™å¤¾")
        return

    # num_workers=2 åŠ é€Ÿè®€å–
    dataset = UpscaleDataset(lr_dir=LR_DIR, hr_dir=HR_DIR)
    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
    
    if len(dataset) == 0:
        print("âŒ Dataset æ˜¯ç©ºçš„")
        return

    # --- å»ºç«‹æ¨¡å‹ ---
    model = UNetSR().to(DEVICE)
    
    # ä½¿ç”¨ L1 Loss (æ¯” MSE æ›´èƒ½ç”¢ç”ŸéŠ³åˆ©é‚Šç·£)
    criterion = nn.L1Loss()
    
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # å­¸ç¿’ç‡æ’ç¨‹ï¼šæ¯ 50 è¼ªè¡°æ¸›ä¸€åŠ
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)

    os.makedirs('models_ckpt', exist_ok=True)

    # --- è¨“ç·´è¿´åœˆ ---
    model.train()
    for epoch in range(NUM_EPOCHS):
        epoch_loss = 0.0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}")
        
        for lr_imgs, hr_imgs in progress_bar:
            lr_imgs, hr_imgs = lr_imgs.to(DEVICE), hr_imgs.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(lr_imgs)
            loss = criterion(outputs, hr_imgs)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            
            # é¡¯ç¤ºç•¶å‰ Loss å’Œ Learning Rate
            current_lr = optimizer.param_groups[0]['lr']
            progress_bar.set_postfix({'loss': f"{loss.item():.6f}", 'lr': f"{current_lr:.6f}"})
            
        # æ›´æ–°å­¸ç¿’ç‡
        scheduler.step()
        
        # æ¯ 20 è¼ªå­˜æª”ä¸€æ¬¡
        if (epoch + 1) % 20 == 0:
            torch.save(model.state_dict(), f'models_ckpt/unet_epoch_{epoch+1}.pth')

    # --- æœ€çµ‚å­˜æª” ---
    final_path = 'models_ckpt/unet_final.pth'
    torch.save(model.state_dict(), final_path)
    print(f"ğŸ‰ UNet Training Finished! Saved to {final_path}")

if __name__ == "__main__":
    train()